{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JsaA8NskpxH6",
        "outputId": "813dc008-2dfa-4327-c2fa-ea040199366f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Accuracy BEFORE Quantization:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-0bc12a06bbea>:40: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load('/content/resnet18.pt', map_location=torch.device('cpu'))\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import torch.utils.data as data\n",
        "import timm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Hyperparameters\n",
        "BATCH_SIZE = 256\n",
        "NUM_BITS = 3\n",
        "INIT_A = 0.55  # Initial power function exponent\n",
        "FIXED_T = 100.5  # Fixed temperature for soft rounding\n",
        "LR = 0.01  # Corrected Learning rate\n",
        "EPOCHS = 10  # Training epochs for optimizing `a`\n",
        "NUM_ITERATIONS = 100  # Per-layer optimization iterations\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, transform=transform, download=True)\n",
        "test_loader = data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# Load Pretrained ResNet18 from timm\n",
        "import resnet18\n",
        "import mobilenetv2\n",
        "import densenet\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = resnet18.resnet18(pretrained=False, device=device)\n",
        "#model = mobilenetv2.mobilenet_v2(pretrained=False, device=device)\n",
        "#model = densenet.densenet121(pretrained=False, device=device)\n",
        "#model.to(device)\n",
        "\n",
        "state_dict = torch.load('/content/resnet18.pt', map_location=torch.device('cpu'))\n",
        "#state_dict = torch.load('/content/mobilenet_v2.pt', map_location=torch.device('cpu'))\n",
        "#state_dict = torch.load('/content/densenet121.pt', map_location=torch.device('cpu'))\n",
        "model.load_state_dict(state_dict, strict=False)\n",
        "model.eval()\n",
        "model.to(device)\n",
        "\n",
        "print(\"Accuracy BEFORE Quantization:\")\n",
        "def evaluate(model, test_loader):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
        "\n",
        "#evaluate(model, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class DAQTanhQuantization(nn.Module):\n",
        "    def __init__(self, num_levels=2**NUM_BITS, init_beta=1.8184, init_c=1.5, fixed_T=FIXED_T, hard_rounding=False):\n",
        "        super().__init__()\n",
        "        self.num_levels = num_levels\n",
        "        self.beta = nn.Parameter(torch.tensor(init_beta, dtype=torch.float32))  # Trainable beta\n",
        "        self.c = nn.Parameter(torch.tensor(init_c, dtype=torch.float32))  # Trainable scaling factor\n",
        "        self.fixed_T = fixed_T\n",
        "        self.hard_rounding = hard_rounding\n",
        "\n",
        "    def forward(self, w):\n",
        "        offset = w.mean()  # Centering the weights\n",
        "        w_shifted = w - offset\n",
        "        EPSILON = 1e-6\n",
        "\n",
        "        # Apply piecewise tanh scaling\n",
        "        tau = 0.5  # Threshold\n",
        "        w_transformed = torch.where(\n",
        "            torch.abs(w_shifted) < tau,\n",
        "            self.beta * w_shifted,  # Linear scaling for small values\n",
        "            self.c * torch.tanh(self.beta * w_shifted) + EPSILON  # Tanh scaling for large values\n",
        "        )\n",
        "\n",
        "        # Normalize weights to [0, 1]\n",
        "        w_min, w_max = w_transformed.min(), w_transformed.max()\n",
        "        w_normalized = (w_transformed - w_min) / (w_max - w_min + 1e-6)  # Avoid division by zero\n",
        "\n",
        "        # Define quantization levels\n",
        "        q_levels = torch.linspace(0, 1, self.num_levels, device=w.device)\n",
        "\n",
        "        if self.hard_rounding:\n",
        "            # HARD ROUNDING: Direct rounding to nearest quantization bin\n",
        "            w_quantized = torch.round(w_normalized * (self.num_levels - 1)) / (self.num_levels - 1)\n",
        "        else:\n",
        "            # SOFT ROUNDING: Distance-aware quantization with softmax\n",
        "            distances = -torch.abs(w_normalized.unsqueeze(-1) - q_levels)  # Negative for softmax\n",
        "            soft_weights = torch.softmax(distances * self.fixed_T, dim=-1)  # Softmax with temperature\n",
        "            w_quantized = (soft_weights * q_levels).sum(dim=-1)\n",
        "\n",
        "        # De-normalize back to original scale\n",
        "        w_dequantized = w_quantized * (w_max - w_min) + w_min\n",
        "\n",
        "        # Clip before inverse transformation to avoid NaN in atanh\n",
        "        w_dequantized = torch.clamp(w_dequantized, -0.9999, 0.9999)\n",
        "        w_dequantized = torch.where(\n",
        "            torch.abs(w_shifted) < tau,\n",
        "            w_dequantized / self.beta,  # Linear inverse for small values\n",
        "            torch.atanh(w_dequantized - EPSILON) / self.beta\n",
        "        ) + offset\n",
        "\n",
        "        return w_dequantized\n"
      ],
      "metadata": {
        "id": "sPoI7LqLHQEf"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DAQPowerQuantization(nn.Module):\n",
        "    def __init__(self, num_levels=2**NUM_BITS, init_a=INIT_A, fixed_T=FIXED_T, hard_rounding=False):\n",
        "        super().__init__()\n",
        "        self.num_levels = num_levels\n",
        "        self.a = nn.Parameter(torch.tensor(init_a, dtype=torch.float32))  # Trainable power exponent\n",
        "        self.fixed_T = fixed_T  # Fixed temperature for soft rounding\n",
        "        self.hard_rounding = hard_rounding  # Toggle for hard rounding\n",
        "\n",
        "    def forward(self, w):\n",
        "        offset = w.mean()  # Centering the weights\n",
        "        w_shifted = w - offset\n",
        "        EPSILON = 1e-6\n",
        "        w_transformed = torch.sign(w_shifted) * (torch.abs(w_shifted) ** self.a) + EPSILON\n",
        "\n",
        "        # Normalize weights to [0, 1]\n",
        "        w_min, w_max = w_transformed.min(), w_transformed.max()\n",
        "        w_normalized = (w_transformed - w_min) / (w_max - w_min + 1e-6)  # Avoid division by zero\n",
        "\n",
        "        # Define quantization levels\n",
        "        q_levels = torch.linspace(0, 1, self.num_levels, device=w.device)\n",
        "\n",
        "        if self.hard_rounding:\n",
        "            # HARD ROUNDING: Direct rounding to nearest quantization bin\n",
        "            w_quantized = torch.round(w_normalized * (self.num_levels - 1)) / (self.num_levels - 1)\n",
        "        else:\n",
        "            # SOFT ROUNDING: Distance-aware quantization with softmax\n",
        "            distances = -torch.abs(w_normalized.unsqueeze(-1) - q_levels)  # Negative for softmax\n",
        "            soft_weights = torch.softmax(distances * self.fixed_T, dim=-1)  # Softmax with temperature\n",
        "            w_quantized = (soft_weights * q_levels).sum(dim=-1)\n",
        "\n",
        "        # De-normalize back to original scale\n",
        "        w_dequantized = w_quantized * (w_max - w_min) + w_min\n",
        "        w_dequantized = (torch.abs(w_dequantized) ** (1/self.a)) * torch.sign(w_dequantized) + offset  # Descale\n",
        "\n",
        "        return w_dequantized\n"
      ],
      "metadata": {
        "id": "nhaFgH8Qp9KB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def optimize_per_layer(model, test_loader, num_iterations=NUM_ITERATIONS, lr=LR):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    updated_state_dict = model.state_dict()\n",
        "    quantization_layers = {}\n",
        "\n",
        "    print(\"Starting per-layer quantization optimization...\")\n",
        "\n",
        "    # Get a batch of test images and labels before optimization\n",
        "    data_iterator = iter(test_loader)\n",
        "    images, labels = next(data_iterator)\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "    # Compute classification loss before optimization\n",
        "    with torch.no_grad():\n",
        "        outputs = model(images)\n",
        "        pre_optimization_loss = classification_loss_fn(outputs, labels).item()\n",
        "\n",
        "    print(f\"Initial Classification Loss Before Optimization: {pre_optimization_loss:.6f}\")\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "        if \"conv\" in name and \"weight\" in name:\n",
        "            print(f\"Optimizing {name}...\")\n",
        "            layer_name = name.replace(\".weight\", \"\")\n",
        "\n",
        "            if layer_name not in temp_activations:\n",
        "                continue  # Skip if activation was not stored\n",
        "\n",
        "            # Initialize differentiable quantization module for this layer\n",
        "            #quantization_layers[layer_name] = DAQPowerQuantization().to(device)\n",
        "            quantization_layers[layer_name] = DAQTanhQuantization().to(device)\n",
        "            quant_layer = quantization_layers[layer_name]\n",
        "\n",
        "            optimizer = optim.Adam(quant_layer.parameters(), lr=lr)\n",
        "            loss_fn = nn.MSELoss()\n",
        "\n",
        "            # Retrieve stored activations\n",
        "            x = temp_activations[layer_name]\n",
        "            original_weight = param.clone().detach()\n",
        "\n",
        "            # Track `a` values\n",
        "            a_tracking = []\n",
        "            prev_class_loss = 100\n",
        "            # Per-layer optimization loop\n",
        "            for iter_idx in range(num_iterations):\n",
        "                optimizer.zero_grad()\n",
        "                quantized_weight = quant_layer(original_weight)  # Apply DAQ quantization\n",
        "\n",
        "                # # Compute output difference\n",
        "                # quantized_output = nn.functional.conv2d(x, quantized_weight, stride=param.shape[2], padding=param.shape[3])\n",
        "                # original_output = nn.functional.conv2d(x, original_weight, stride=param.shape[2], padding=param.shape[3])\n",
        "                # Detect Depthwise Convolution\n",
        "                groups = param.shape[0] if param.shape[1] == 1 else 1  # Depthwise conv fix\n",
        "\n",
        "                # Compute output difference\n",
        "                quantized_output = nn.functional.conv2d(\n",
        "                    x, quantized_weight, stride=param.shape[2], padding=param.shape[3], groups=groups\n",
        "                )\n",
        "                original_output = nn.functional.conv2d(\n",
        "                    x, original_weight, stride=param.shape[2], padding=param.shape[3], groups=groups\n",
        "                )\n",
        "\n",
        "                # Reconstruction loss\n",
        "                reconstruction_loss = loss_fn(quantized_output, original_output)\n",
        "\n",
        "                if reconstruction_loss < 2e-7:\n",
        "                    break\n",
        "\n",
        "                # Get a batch of test images and labels\n",
        "                for images, labels in test_loader:\n",
        "                    images, labels = images.to(device), labels.to(device)\n",
        "                    break  # Use only one batch\n",
        "\n",
        "                # Temporarily replace model weight with quantized weight (without modifying in-place)\n",
        "                with torch.no_grad():\n",
        "                    temp_weight = quantized_weight.detach().clone()\n",
        "                    param_backup = param.data.clone()\n",
        "                    param.data.copy_(temp_weight)\n",
        "\n",
        "                # Compute classification loss with quantized model\n",
        "                classification_loss = classification_loss_fn(model(images), labels)\n",
        "                if prev_class_loss < classification_loss:\n",
        "                    break\n",
        "                prev_class_loss = classification_loss\n",
        "                # Restore original weight after loss computation\n",
        "                with torch.no_grad():\n",
        "                    param.data.copy_(param_backup)\n",
        "\n",
        "                # Compute total loss\n",
        "                final_loss = 0.1* reconstruction_loss + 0.9 * classification_loss\n",
        "                final_loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                a_tracking.append(quant_layer.beta.item())\n",
        "\n",
        "                if iter_idx % 10== 0:\n",
        "                    #print(classification_loss.item(), reconstruction_loss.item())\n",
        "                    print(f\"Iter {iter_idx}: classification_loss = {classification_loss.item():.8f}, reconstruction_loss = {reconstruction_loss.item():.4f}\")\n",
        "                    print(f\"Iter {iter_idx}: Loss = {final_loss.item():.8f}, b = {quant_layer.beta.item():.4f}\")\n",
        "\n",
        "            updated_state_dict[name] = quant_layer(original_weight).detach()\n",
        "\n",
        "    model.load_state_dict(updated_state_dict)\n",
        "    print(\"Per-layer optimization complete.\")\n"
      ],
      "metadata": {
        "id": "ggs_0jalp-Aw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply Hard Rounding for Final Evaluation\n",
        "def apply_hard_rounding(model):\n",
        "    print(\"Applying Hard Rounding for Final Evaluation...\")\n",
        "    for name, param in model.named_parameters():\n",
        "        if \"conv\" in name and \"weight\" in name:\n",
        "            layer = DAQPowerQuantization(hard_rounding=True).to(device)\n",
        "            param.data = layer(param).detach()  # Apply hard rounding\n"
      ],
      "metadata": {
        "id": "a-2-D2gvp_2Y"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Cross-Entropy Loss for Classification\n",
        "classification_loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# ====== Store Layer Activations for Per-Layer Optimization ======\n",
        "temp_activations = {}\n",
        "\n",
        "def activation_hook(layer_name):\n",
        "    def hook(module, input, output):\n",
        "        temp_activations[layer_name] = input[0].detach()\n",
        "    return hook\n",
        "\n",
        "# Register hooks for convolutional layers\n",
        "for name, layer in model.named_modules():\n",
        "    if isinstance(layer, nn.Conv2d):\n",
        "        layer.register_forward_hook(activation_hook(name))\n",
        "# First, optimize using soft rounding\n",
        "optimize_per_layer(model, test_loader)\n",
        "\n",
        "# Then, apply hard rounding before final evaluation\n",
        "#apply_hard_rounding(model)\n",
        "\n",
        "# Evaluate Model After Hard Rounding\n",
        "evaluate(model, test_loader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Rr4bh0cqEzo",
        "outputId": "b13e003e-1a8b-46dc-de1f-d7817902c1fc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting per-layer quantization optimization...\n",
            "Initial Classification Loss Before Optimization: 0.526322\n",
            "Optimizing conv1.weight...\n",
            "Iter 0: classification_loss = 0.79476923, reconstruction_loss = 0.0046\n",
            "Iter 0: Loss = 0.71575695, b = 1.8133\n",
            "Optimizing layer1.0.conv1.weight...\n",
            "Iter 0: classification_loss = 1.93208408, reconstruction_loss = 0.0001\n",
            "Iter 0: Loss = 1.73888171, b = 1.8168\n",
            "Optimizing layer1.0.conv2.weight...\n",
            "Iter 0: classification_loss = 2.12270570, reconstruction_loss = 0.0000\n",
            "Iter 0: Loss = 1.91043568, b = 1.8185\n",
            "Optimizing layer1.1.conv1.weight...\n",
            "Iter 0: classification_loss = 2.06956673, reconstruction_loss = 0.0000\n",
            "Iter 0: Loss = 1.86261165, b = 1.8191\n",
            "Optimizing layer1.1.conv2.weight...\n",
            "Iter 0: classification_loss = 2.11616588, reconstruction_loss = 0.0000\n",
            "Iter 0: Loss = 1.90454936, b = 1.8184\n",
            "Optimizing layer2.0.conv1.weight...\n",
            "Iter 0: classification_loss = 2.13264680, reconstruction_loss = 0.0000\n",
            "Iter 0: Loss = 1.91938293, b = 1.8183\n",
            "Optimizing layer2.0.conv2.weight...\n",
            "Iter 0: classification_loss = 1.96479118, reconstruction_loss = 0.0000\n",
            "Iter 0: Loss = 1.76831210, b = 1.8184\n",
            "Optimizing layer2.1.conv1.weight...\n",
            "Iter 0: classification_loss = 1.93349290, reconstruction_loss = 0.0000\n",
            "Iter 0: Loss = 1.74014366, b = 1.8184\n",
            "Optimizing layer2.1.conv2.weight...\n",
            "Iter 0: classification_loss = 2.00450325, reconstruction_loss = 0.0000\n",
            "Iter 0: Loss = 1.80405283, b = 1.8184\n",
            "Optimizing layer3.0.conv1.weight...\n",
            "Iter 0: classification_loss = 2.10645056, reconstruction_loss = 0.0000\n",
            "Iter 0: Loss = 1.89580560, b = 1.8185\n",
            "Optimizing layer3.0.conv2.weight...\n",
            "Iter 0: classification_loss = 2.27377892, reconstruction_loss = 0.0000\n",
            "Iter 0: Loss = 2.04640102, b = 1.8184\n",
            "Optimizing layer3.1.conv1.weight...\n",
            "Iter 0: classification_loss = 2.24443364, reconstruction_loss = 0.0000\n",
            "Iter 0: Loss = 2.01999021, b = 1.8185\n",
            "Optimizing layer3.1.conv2.weight...\n",
            "Iter 0: classification_loss = 2.68537807, reconstruction_loss = 0.0000\n",
            "Iter 0: Loss = 2.41684031, b = 1.8183\n",
            "Optimizing layer4.0.conv1.weight...\n",
            "Iter 0: classification_loss = 2.60391808, reconstruction_loss = 0.0000\n",
            "Iter 0: Loss = 2.34352612, b = 1.8184\n",
            "Optimizing layer4.0.conv2.weight...\n",
            "Iter 0: classification_loss = 2.70161033, reconstruction_loss = 0.0000\n",
            "Iter 0: Loss = 2.43144965, b = 1.8179\n",
            "Optimizing layer4.1.conv1.weight...\n",
            "Iter 0: classification_loss = 2.68406439, reconstruction_loss = 0.0002\n",
            "Iter 0: Loss = 2.41567969, b = 1.8132\n",
            "Iter 10: classification_loss = 2.68405914, reconstruction_loss = 0.0002\n",
            "Iter 10: Loss = 2.41567492, b = 1.7609\n",
            "Iter 20: classification_loss = 2.68405414, reconstruction_loss = 0.0002\n",
            "Iter 20: Loss = 2.41567039, b = 1.7073\n",
            "Iter 30: classification_loss = 2.68404865, reconstruction_loss = 0.0002\n",
            "Iter 30: Loss = 2.41566539, b = 1.6517\n",
            "Iter 40: classification_loss = 2.68404245, reconstruction_loss = 0.0002\n",
            "Iter 40: Loss = 2.41565990, b = 1.5939\n",
            "Iter 50: classification_loss = 2.68403482, reconstruction_loss = 0.0002\n",
            "Iter 50: Loss = 2.41565299, b = 1.5333\n",
            "Iter 60: classification_loss = 2.68402696, reconstruction_loss = 0.0002\n",
            "Iter 60: Loss = 2.41564584, b = 1.4695\n",
            "Iter 70: classification_loss = 2.68401766, reconstruction_loss = 0.0002\n",
            "Iter 70: Loss = 2.41563749, b = 1.4020\n",
            "Iter 80: classification_loss = 2.68400669, reconstruction_loss = 0.0002\n",
            "Iter 80: Loss = 2.41562772, b = 1.3302\n",
            "Iter 90: classification_loss = 2.68399358, reconstruction_loss = 0.0002\n",
            "Iter 90: Loss = 2.41561580, b = 1.2533\n",
            "Optimizing layer4.1.conv2.weight...\n",
            "Iter 0: classification_loss = 2.26439476, reconstruction_loss = 0.0012\n",
            "Iter 0: Loss = 2.03807473, b = 1.8250\n",
            "Per-layer optimization complete.\n",
            "Test Accuracy: 37.21%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#model = resnet18.resnet18(pretrained=False, device=device)\n",
        "#model = mobilenetv2.mobilenet_v2(pretrained=False, device=device)\n",
        "model = densenet.densenet121(pretrained=False, device=device)\n",
        "model.to(device)\n",
        "\n",
        "#state_dict = torch.load('/content/resnet18.pt', map_location=torch.device('cpu'))\n",
        "#state_dict = torch.load('/content/mobilenet_v2.pt', map_location=torch.device('cpu'))\n",
        "state_dict = torch.load('/content/densenet121.pt', map_location=torch.device('cpu'))\n",
        "model.load_state_dict(state_dict, strict=False)\n",
        "model.eval()\n",
        "model.to(device)\n",
        "\n",
        "def uniform_quantization(tensor, num_bits=NUM_BITS):\n",
        "    min_val, max_val = tensor.min(), tensor.max()\n",
        "    scale = (max_val - min_val) / (2 ** num_bits - 1)\n",
        "    quantized_tensor = torch.round((tensor - min_val) / scale) * scale + min_val\n",
        "    return quantized_tensor\n",
        "\n",
        "# Apply Uniform Quantization to Conv Layers Only\n",
        "for name, param in model.named_parameters():\n",
        "    if \"conv\" in name and \"weight\" in name:\n",
        "        param.data = uniform_quantization(param.data)\n",
        "\n",
        "evaluate(model, test_loader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wyi-P_eq0T7l",
        "outputId": "27f5fec3-3df1-4ec4-8a98-3accb2ef0275"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-5ecf55dbae62>:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load('/content/densenet121.pt', map_location=torch.device('cpu'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 10.07%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class DAQLogQuantization(nn.Module):\n",
        "    def __init__(self, num_levels=2**NUM_BITS, init_base=1.44, fixed_T=FIXED_T, hard_rounding=False):\n",
        "        super().__init__()\n",
        "        self.num_levels = num_levels\n",
        "        self.base = nn.Parameter(torch.tensor(init_base, dtype=torch.float32))  # Trainable log base\n",
        "        self.fixed_T = fixed_T  # Fixed temperature for soft rounding\n",
        "        self.hard_rounding = hard_rounding  # Toggle for hard rounding\n",
        "\n",
        "    def forward(self, w):\n",
        "        offset = w.mean()  # Centering the weights\n",
        "        w_shifted = w - offset\n",
        "        EPSILON = 1e-6\n",
        "        w_transformed = torch.sign(w_shifted) * torch.log1p(torch.abs(w_shifted)) / torch.log(self.base) + EPSILON\n",
        "\n",
        "        # Normalize weights to [0, 1]\n",
        "        w_min, w_max = w_transformed.min(), w_transformed.max()\n",
        "        w_normalized = (w_transformed - w_min) / (w_max - w_min + 1e-6)  # Avoid division by zero\n",
        "\n",
        "        # Define quantization levels\n",
        "        q_levels = torch.linspace(0, 1, self.num_levels, device=w.device)\n",
        "\n",
        "        if self.hard_rounding:\n",
        "            # HARD ROUNDING: Direct rounding to nearest quantization bin\n",
        "            w_quantized = torch.round(w_normalized * (self.num_levels - 1)) / (self.num_levels - 1)\n",
        "        else:\n",
        "            # SOFT ROUNDING: Distance-aware quantization with softmax\n",
        "            distances = -torch.abs(w_normalized.unsqueeze(-1) - q_levels)  # Negative for softmax\n",
        "            soft_weights = torch.softmax(distances * self.fixed_T, dim=-1)  # Softmax with temperature\n",
        "            w_quantized = (soft_weights * q_levels).sum(dim=-1)\n",
        "\n",
        "        # De-normalize back to original scale\n",
        "        w_dequantized = w_quantized * (w_max - w_min) + w_min\n",
        "        w_dequantized = torch.exp(w_dequantized * torch.log(self.base)) - 1  # Descale using inverse log\n",
        "        w_dequantized = w_dequantized * torch.sign(w_shifted) + offset\n",
        "\n",
        "        return w_dequantized\n"
      ],
      "metadata": {
        "id": "REwagjFpBnWk"
      },
      "execution_count": 2,
      "outputs": []
    }
  ]
}