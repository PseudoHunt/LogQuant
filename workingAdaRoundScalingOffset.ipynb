{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iyRbGlDLIyuy",
        "outputId": "85f923d5-3cab-430e-d79f-4730edf08328"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-35386356e81b>:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load('/content/resnet18.pt', map_location=torch.device('cpu'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Accuracy of the network on the 10000 test images: 92 %\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Import the ResNet18 class from your resnet18.py file\n",
        "\n",
        "# Import the model from the script\n",
        "import resnet18\n",
        "\n",
        "# Set device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load ResNet-18 model\n",
        "model = resnet18.resnet18(pretrained=False, device=device)  # Don't use default pretrained weights\n",
        "model.to(device)\n",
        "\n",
        "# Load the state dict from the .pt file\n",
        "state_dict = torch.load('/content/resnet18.pt', map_location=torch.device('cpu'))\n",
        "\n",
        "# Load the saved weights into the model, using strict=False to ignore mismatches\n",
        "model.load_state_dict(state_dict, strict=False)\n",
        "\n",
        "# Set the model to evaluation mode and move to device\n",
        "model.eval()\n",
        "model.to(device)\n",
        "\n",
        "# Define the transformations for CIFAR-10\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "# Load the CIFAR-10 test dataset\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "\n",
        "# Evaluate the model\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate Model after Quantization\n",
        "def evaluate(model, test_loader):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    print(f\"Test Accuracy after Quantization: {100 * correct / total:.2f}%\")"
      ],
      "metadata": {
        "id": "-9f2MYbJkgoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_BITS = 4\n",
        "def uniform_quantization(tensor, num_bits=NUM_BITS):\n",
        "    min_val, max_val = tensor.min(), tensor.max()\n",
        "    scale = (max_val - min_val) / (2 ** num_bits - 1)\n",
        "    quantized_tensor = torch.round((tensor - min_val) / scale) * scale + min_val\n",
        "    return quantized_tensor\n",
        "\n",
        "\n",
        "# Apply Uniform Quantization\n",
        "for name, param in model.named_parameters():\n",
        "    if \"weight\" in name:\n",
        "        print(name)\n",
        "        param.data = uniform_quantization(param.data)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NaDYLw0I44R",
        "outputId": "abb81d69-9284-4b6f-ae2f-6609e43b87c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "conv1.weight\n",
            "bn1.weight\n",
            "layer1.0.conv1.weight\n",
            "layer1.0.bn1.weight\n",
            "layer1.0.conv2.weight\n",
            "layer1.0.bn2.weight\n",
            "layer1.1.conv1.weight\n",
            "layer1.1.bn1.weight\n",
            "layer1.1.conv2.weight\n",
            "layer1.1.bn2.weight\n",
            "layer2.0.conv1.weight\n",
            "layer2.0.bn1.weight\n",
            "layer2.0.conv2.weight\n",
            "layer2.0.bn2.weight\n",
            "layer2.0.downsample.0.weight\n",
            "layer2.0.downsample.1.weight\n",
            "layer2.1.conv1.weight\n",
            "layer2.1.bn1.weight\n",
            "layer2.1.conv2.weight\n",
            "layer2.1.bn2.weight\n",
            "layer3.0.conv1.weight\n",
            "layer3.0.bn1.weight\n",
            "layer3.0.conv2.weight\n",
            "layer3.0.bn2.weight\n",
            "layer3.0.downsample.0.weight\n",
            "layer3.0.downsample.1.weight\n",
            "layer3.1.conv1.weight\n",
            "layer3.1.bn1.weight\n",
            "layer3.1.conv2.weight\n",
            "layer3.1.bn2.weight\n",
            "layer4.0.conv1.weight\n",
            "layer4.0.bn1.weight\n",
            "layer4.0.conv2.weight\n",
            "layer4.0.bn2.weight\n",
            "layer4.0.downsample.0.weight\n",
            "layer4.0.downsample.1.weight\n",
            "layer4.1.conv1.weight\n",
            "layer4.1.bn1.weight\n",
            "layer4.1.conv2.weight\n",
            "layer4.1.bn2.weight\n",
            "fc.weight\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate Model after AdaRound Quantization\n",
        "def evaluate(model, test_loader):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    print(f\"Test Accuracy after Uniform quantization: {100 * correct / total:.2f}%\")\n",
        "evaluate(model, testloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGDVvesNJdRm",
        "outputId": "bf6b4340-7a4e-4b10-b228-9e64a0449bb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy after Uniform quantization: 89.89%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import torch.utils.data as data\n",
        "import timm\n",
        "import numpy as np\n",
        "\n",
        "# Hyperparameters\n",
        "BATCH_SIZE = 128\n",
        "NUM_BITS = 2\n",
        "BETA = 2.0\n",
        "LAMBDA = 0.01  # Regularization weight\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, transform=transform, download=True)\n",
        "\n",
        "test_loader = data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# Load Pretrained ResNet18 from timm\n",
        "import resnet18\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = resnet18.resnet18(pretrained=False, device=device)\n",
        "model.to(device)\n",
        "\n",
        "state_dict = torch.load('/content/resnet18.pt', map_location=torch.device('cpu'))\n",
        "model.load_state_dict(state_dict, strict=False)\n",
        "model.eval()\n",
        "model.to(device)\n",
        "# Evaluate Model after Quantization\n",
        "def evaluate(model, test_loader):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    print(f\"Test Accuracy after Quantization: {100 * correct / total:.2f}%\")\n",
        "evaluate(model, test_loader)\n",
        "# AdaRound: Adaptive Rounding Quantization with V Optimization\n",
        "\n",
        "def adaround_round(tensor, v, num_bits=NUM_BITS, beta=BETA, lambda_reg=LAMBDA):\n",
        "    scale = (tensor.max() - tensor.min()) / (2 ** num_bits - 1)\n",
        "    h = torch.sigmoid(beta * v)\n",
        "    rounded_tensor = torch.floor(tensor / scale) + h\n",
        "    quantized_tensor = (rounded_tensor * scale).clamp(tensor.min(), tensor.max())\n",
        "    regularization = lambda_reg * torch.sum(1 - torch.abs(2 * h - 1))\n",
        "    return quantized_tensor, regularization\n",
        "\n",
        "def uniform_quantization(tensor, num_bits=NUM_BITS):\n",
        "    min_val, max_val = tensor.min(), tensor.max()\n",
        "    scale = (max_val - min_val) / (2 ** num_bits - 1)\n",
        "    quantized_tensor = torch.round((tensor - min_val) / scale) * scale + min_val\n",
        "    return quantized_tensor\n",
        "\n",
        "def get_previous_layer_output(model, images, current_layer_name):\n",
        "    layers = list(model.named_modules())\n",
        "    prev_layer_name = None\n",
        "    for i, (layer_name, _) in enumerate(layers):\n",
        "        if layer_name == current_layer_name and i > 0:\n",
        "            prev_layer_name = layers[i - 1][0]\n",
        "            break\n",
        "    if prev_layer_name is None:\n",
        "        raise ValueError(f\"Could not determine previous layer for {current_layer_name}\")\n",
        "\n",
        "    activation = {}\n",
        "    def hook_fn(module, input, output):\n",
        "        activation[prev_layer_name] = output.detach()\n",
        "\n",
        "    handle = dict(model.named_modules())[prev_layer_name].register_forward_hook(hook_fn)\n",
        "    with torch.no_grad():\n",
        "        model(images)\n",
        "    handle.remove()\n",
        "    return activation[prev_layer_name]\n",
        "\n",
        "def optimize_adaround(model, test_loader, num_iterations=1000, lr=0.01):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    first_conv = True  # Track first convolutional layer\n",
        "    for name, param in model.named_parameters():\n",
        "        if \"conv\" in name and \"weight\" in name:  # Only quantize convolutional layers\n",
        "            if first_conv:\n",
        "                first_conv = False\n",
        "                continue  # Skip first convolutional layer\n",
        "            v = torch.nn.Parameter(torch.zeros_like(param, device=device))\n",
        "            original_weight = param.clone().detach()\n",
        "            optimizer_v = optim.Adam([v], lr=lr)\n",
        "\n",
        "            for images, _ in test_loader:\n",
        "                images = images.to(device)\n",
        "                with torch.no_grad():\n",
        "                    x = get_previous_layer_output(model, images, name.replace(\"weight\", \"\"))\n",
        "                break  # Only need a single batch for optimization\n",
        "\n",
        "            for _ in range(num_iterations):\n",
        "                optimizer_v.zero_grad()\n",
        "                quantized_weight, reg_loss = adaround_round(original_weight, v)\n",
        "\n",
        "                # Ensure correct stride and padding based on original layer\n",
        "                stride = model.state_dict()[name.replace(\"weight\", \"stride\")]\n",
        "                padding = model.state_dict()[name.replace(\"weight\", \"padding\")]\n",
        "\n",
        "                quantized_output = nn.functional.conv2d(x, quantized_weight, stride=stride, padding=padding)\n",
        "                original_output = nn.functional.conv2d(x, original_weight, stride=stride, padding=padding)\n",
        "\n",
        "                loss = torch.norm(original_output - quantized_output, p='fro') ** 2 + reg_loss\n",
        "                loss.backward()\n",
        "                optimizer_v.step()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                param.copy_(adaround_round(original_weight, v)[0])\n",
        "\n",
        "    print(\"AdaRound optimization complete.\")\n",
        "\n",
        "# Apply AdaRound Quantization\n",
        "optimize_adaround(model, test_loader)\n",
        "\n",
        "# # Apply Uniform Quantization to Conv Layers Only\n",
        "# for name, param in model.named_parameters():\n",
        "#     if \"conv\" in name and \"weight\" in name:\n",
        "#         param.data = uniform_quantization(param.data)\n",
        "\n",
        "\n",
        "\n",
        "evaluate(model, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "id": "VzPmvaBlfifs",
        "outputId": "e44d63a7-829b-4ace-e168-7bed401e9703"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-1918e0eda4f3>:34: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load('/content/resnet18.pt', map_location=torch.device('cpu'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy after Quantization: 86.53%\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Could not determine previous layer for layer1.0.conv1.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-1918e0eda4f3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;31m# Apply AdaRound Quantization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m \u001b[0moptimize_adaround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;31m# # Apply Uniform Quantization to Conv Layers Only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-1918e0eda4f3>\u001b[0m in \u001b[0;36moptimize_adaround\u001b[0;34m(model, test_loader, num_iterations, lr)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m                     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_previous_layer_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"weight\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m                 \u001b[0;32mbreak\u001b[0m  \u001b[0;31m# Only need a single batch for optimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-1918e0eda4f3>\u001b[0m in \u001b[0;36mget_previous_layer_output\u001b[0;34m(model, images, current_layer_name)\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprev_layer_name\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Could not determine previous layer for {current_layer_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Could not determine previous layer for layer1.0.conv1."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import torch.utils.data as data\n",
        "import timm\n",
        "import numpy as np\n",
        "\n",
        "# Hyperparameters\n",
        "BATCH_SIZE = 128\n",
        "NUM_BITS = 3\n",
        "BETA = 2.0\n",
        "LAMBDA = 0.01  # Regularization weight\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, transform=transform, download=True)\n",
        "\n",
        "test_loader = data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# Load Pretrained ResNet18 from timm\n",
        "import resnet18\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = resnet18.resnet18(pretrained=False, device=device)\n",
        "model.to(device)\n",
        "\n",
        "state_dict = torch.load('/content/resnet18.pt', map_location=torch.device('cpu'))\n",
        "model.load_state_dict(state_dict, strict=False)\n",
        "model.eval()\n",
        "model.to(device)\n",
        "\n",
        "# AdaRound: Adaptive Rounding Quantization with V Optimization\n",
        "\n",
        "def adaround_round(tensor, v, num_bits=NUM_BITS, beta=BETA, lambda_reg=LAMBDA):\n",
        "    scale = (tensor.max() - tensor.min()) / (2 ** num_bits - 1)\n",
        "    h = torch.sigmoid(beta * v)\n",
        "    rounded_tensor = torch.floor(tensor / scale) + h\n",
        "    quantized_tensor = (rounded_tensor * scale).clamp(tensor.min(), tensor.max())\n",
        "    regularization = lambda_reg * torch.sum(1 - torch.abs(2 * h - 1))\n",
        "    return quantized_tensor, regularization\n",
        "\n",
        "def uniform_quantization(tensor, num_bits=NUM_BITS):\n",
        "    min_val, max_val = tensor.min(), tensor.max()\n",
        "    scale = (max_val - min_val) / (2 ** num_bits - 1)\n",
        "    quantized_tensor = torch.round((tensor - min_val) / scale) * scale + min_val\n",
        "    return quantized_tensor\n",
        "\n",
        "def get_previous_layer_output(model, images, current_layer_name):\n",
        "    layers = list(model.named_modules())\n",
        "    prev_layer_name = None\n",
        "    for i, (layer_name, _) in enumerate(layers):\n",
        "        if layer_name == current_layer_name and i > 0:\n",
        "            prev_layer_name = layers[i - 1][0]\n",
        "            break\n",
        "    if prev_layer_name is None:\n",
        "        raise ValueError(f\"Could not determine previous layer for {current_layer_name}\")\n",
        "\n",
        "    activation = {}\n",
        "    def hook_fn(module, input, output):\n",
        "        activation[prev_layer_name] = output.detach()\n",
        "\n",
        "    handle = dict(model.named_modules())[prev_layer_name].register_forward_hook(hook_fn)\n",
        "    with torch.no_grad():\n",
        "        model(images)\n",
        "    handle.remove()\n",
        "    return activation[prev_layer_name]\n",
        "\n",
        "def optimize_adaround(model, test_loader, num_iterations=1000, lr=0.01):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "        if \"conv\" in name and \"weight\" in name and not \"conv1\":  # Only quantize convolutional layers\n",
        "            v = torch.nn.Parameter(torch.zeros_like(param, device=device))\n",
        "            original_weight = param.clone().detach()\n",
        "            optimizer_v = optim.Adam([v], lr=lr)\n",
        "\n",
        "            for images, _ in test_loader:\n",
        "                images = images.to(device)\n",
        "                with torch.no_grad():\n",
        "                    x = get_previous_layer_output(model, images, name.replace(\"weight\", \"\"))\n",
        "                break  # Only need a single batch for optimization\n",
        "\n",
        "            for _ in range(num_iterations):\n",
        "                optimizer_v.zero_grad()\n",
        "                quantized_weight, reg_loss = adaround_round(original_weight, v)\n",
        "\n",
        "                # Ensure correct stride and padding based on original layer\n",
        "                stride = model.state_dict()[name.replace(\"weight\", \"stride\")]\n",
        "                padding = model.state_dict()[name.replace(\"weight\", \"padding\")]\n",
        "\n",
        "                quantized_output = nn.functional.conv2d(x, quantized_weight, stride=stride, padding=padding)\n",
        "                original_output = nn.functional.conv2d(x, original_weight, stride=stride, padding=padding)\n",
        "\n",
        "                loss = torch.norm(original_output - quantized_output, p='fro') ** 2 + reg_loss\n",
        "                loss.backward()\n",
        "                optimizer_v.step()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                param.copy_(adaround_round(original_weight, v)[0])\n",
        "\n",
        "    print(\"AdaRound optimization complete.\")\n",
        "\n",
        "# Apply AdaRound Quantization\n",
        "#optimize_adaround(model, test_loader)\n",
        "\n",
        "# Apply Uniform Quantization to Conv Layers Only\n",
        "for name, param in model.named_parameters():\n",
        "    if \"conv\" in name and \"weight\" in name:\n",
        "        param.data = uniform_quantization(param.data)\n",
        "\n",
        "# Evaluate Model after Quantization\n",
        "def evaluate(model, test_loader):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    print(f\"Test Accuracy after Quantization: {100 * correct / total:.2f}%\")\n",
        "\n",
        "evaluate(model, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V18DltUUkoKv",
        "outputId": "e86c157b-a9f7-4abb-d6b5-c3f8cc817848"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-d46577d72080>:34: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load('/content/resnet18.pt', map_location=torch.device('cpu'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy after Quantization: 22.76%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import torch.utils.data as data\n",
        "import timm\n",
        "import numpy as np\n",
        "\n",
        "# Hyperparameters\n",
        "BATCH_SIZE = 128\n",
        "NUM_BITS = 4\n",
        "BETA = 2.0\n",
        "LAMBDA = 0.01  # Regularization weight\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, transform=transform, download=True)\n",
        "\n",
        "test_loader = data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# Load Pretrained ResNet18 from timm\n",
        "import resnet18\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = resnet18.resnet18(pretrained=False, device=device)\n",
        "model.to(device)\n",
        "\n",
        "state_dict = torch.load('/content/resnet18.pt', map_location=torch.device('cpu'))\n",
        "model.load_state_dict(state_dict, strict=False)\n",
        "model.eval()\n",
        "model.to(device)\n",
        "\n",
        "# AdaRound: Adaptive Rounding Quantization with V Optimization\n",
        "\n",
        "def adaround_round(tensor, v, num_bits=NUM_BITS, beta=BETA, lambda_reg=LAMBDA):\n",
        "    scale = (tensor.max() - tensor.min()) / (2 ** num_bits - 1)\n",
        "    h = torch.sigmoid(beta * v)\n",
        "    rounded_tensor = torch.floor(tensor / scale) + h\n",
        "    quantized_tensor = (rounded_tensor * scale).clamp(tensor.min(), tensor.max())\n",
        "    regularization = lambda_reg * torch.sum(1 - torch.abs(2 * h - 1))\n",
        "    return quantized_tensor, regularization\n",
        "\n",
        "def uniform_quantization(tensor, num_bits=NUM_BITS):\n",
        "    min_val, max_val = tensor.min(), tensor.max()\n",
        "    scale = (max_val - min_val) / (2 ** num_bits - 1)\n",
        "    quantized_tensor = torch.round((tensor - min_val) / scale) * scale + min_val\n",
        "    return quantized_tensor\n",
        "\n",
        "def get_previous_layer_output(model, images, current_layer_name):\n",
        "    layers = list(model.named_modules())\n",
        "    prev_layer_name = None\n",
        "    for i, (layer_name, _) in enumerate(layers):\n",
        "        if layer_name == current_layer_name and i > 0:\n",
        "            prev_layer_name = layers[i - 1][0]\n",
        "            break\n",
        "    if prev_layer_name is None:\n",
        "        raise ValueError(f\"Could not determine previous layer for {current_layer_name}\")\n",
        "\n",
        "    activation = {}\n",
        "    def hook_fn(module, input, output):\n",
        "        activation[prev_layer_name] = output.detach()\n",
        "\n",
        "    handle = dict(model.named_modules())[prev_layer_name].register_forward_hook(hook_fn)\n",
        "    with torch.no_grad():\n",
        "        model(images)\n",
        "    handle.remove()\n",
        "    return activation[prev_layer_name]\n",
        "\n",
        "def optimize_adaround(model, test_loader, num_iterations=1000, lr=0.01):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    updated_state_dict = model.state_dict()\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "        if \"conv\" in name and \"weight\" in name and not 'conv1':  # Only quantize convolutional layers\n",
        "            print(name)\n",
        "            v = torch.nn.Parameter(torch.zeros_like(param, device=device))\n",
        "            original_weight = param.clone().detach()\n",
        "            optimizer_v = optim.Adam([v], lr=lr)\n",
        "\n",
        "            for images, _ in test_loader:\n",
        "                images = images.to(device)\n",
        "                with torch.no_grad():\n",
        "                    x = get_previous_layer_output(model, images, name.replace(\"weight\", \"\"))\n",
        "                break  # Only need a single batch for optimization\n",
        "\n",
        "            for _ in range(num_iterations):\n",
        "                optimizer_v.zero_grad()\n",
        "                quantized_weight, reg_loss = adaround_round(original_weight, v)\n",
        "\n",
        "                stride = model.state_dict()[name.replace(\"weight\", \"stride\")]\n",
        "                padding = model.state_dict()[name.replace(\"weight\", \"padding\")]\n",
        "\n",
        "                quantized_output = nn.functional.conv2d(x, quantized_weight, stride=stride, padding=padding)\n",
        "                original_output = nn.functional.conv2d(x, original_weight, stride=stride, padding=padding)\n",
        "\n",
        "                loss = torch.norm(original_output - quantized_output, p='fro') ** 2 + reg_loss\n",
        "                loss.backward()\n",
        "                optimizer_v.step()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                updated_state_dict[name] = adaround_round(original_weight, v)[0]\n",
        "\n",
        "    model.load_state_dict(updated_state_dict)  # Ensure model actually uses quantized weights\n",
        "    print(\"AdaRound optimization complete. Model weights updated.\")\n",
        "\n",
        "# Apply AdaRound Quantization\n",
        "optimize_adaround(model, test_loader)\n",
        "\n",
        "# # Apply Uniform Quantization to Conv Layers Only\n",
        "# for name, param in model.named_parameters():\n",
        "#     if \"conv\" in name and \"weight\" in name:\n",
        "#         param.data = uniform_quantization(param.data)\n",
        "\n",
        "# Reload state dict to ensure updated weights are used\n",
        "model.load_state_dict(model.state_dict())\n",
        "\n",
        "# Evaluate Model after Quantization\n",
        "def evaluate(model, test_loader):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    print(f\"Test Accuracy after Quantization: {100 * correct / total:.2f}%\")\n",
        "\n",
        "evaluate(model, test_loader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FTRYv35UoXob",
        "outputId": "915d8f97-0127-47b6-9f40-7e1448ea9b55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-8e89e4c1aab1>:34: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load('/content/resnet18.pt', map_location=torch.device('cpu'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AdaRound optimization complete. Model weights updated.\n",
            "Test Accuracy after Quantization: 86.53%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import torch.utils.data as data\n",
        "import timm\n",
        "import numpy as np\n",
        "\n",
        "# Hyperparameters\n",
        "BATCH_SIZE = 128\n",
        "NUM_BITS = 4\n",
        "BETA = 2.0\n",
        "LAMBDA = 0.0001  # Regularization weight\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, transform=transform, download=True)\n",
        "\n",
        "test_loader = data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# Load Pretrained ResNet18 from timm\n",
        "import resnet18\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = resnet18.resnet18(pretrained=False, device=device)\n",
        "model.to(device)\n",
        "\n",
        "state_dict = torch.load('/content/resnet18.pt', map_location=torch.device('cpu'))\n",
        "model.load_state_dict(state_dict, strict=False)\n",
        "model.eval()\n",
        "model.to(device)\n",
        "# Evaluate Model after Quantization\n",
        "def evaluate(model, test_loader):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    print(f\"Test Accuracy after Quantization: {100 * correct / total:.2f}%\")\n",
        "\n",
        "evaluate(model, test_loader)\n",
        "\n",
        "\n",
        "\n",
        "# Store layer activations\n",
        "temp_activations = {}\n",
        "\n",
        "def activation_hook(layer_name):\n",
        "    def hook(module, input, output):\n",
        "        temp_activations[layer_name] = input[0].detach()\n",
        "    return hook\n",
        "\n",
        "# Register hooks for all convolutional layers, skipping the first conv layer\n",
        "first_conv = True\n",
        "for name, layer in model.named_modules():\n",
        "    if isinstance(layer, nn.Conv2d):\n",
        "        if first_conv:\n",
        "            first_conv = False\n",
        "            continue  # Skip the first convolutional layer\n",
        "        layer.register_forward_hook(activation_hook(name))\n",
        "\n",
        "# AdaRound: Adaptive Rounding Quantization with V Optimization\n",
        "\n",
        "def adaround_round(tensor, v, num_bits=NUM_BITS, beta=BETA, lambda_reg=LAMBDA):\n",
        "    scale = (tensor.max() - tensor.min()) / (2 ** num_bits - 1)\n",
        "    h = torch.sigmoid(beta * v)\n",
        "    rounded_tensor = torch.floor(tensor / scale) + h\n",
        "    quantized_tensor = (rounded_tensor * scale).clamp(tensor.min(), tensor.max())\n",
        "    regularization = lambda_reg * torch.sum(1 - torch.abs(2 * h - 1))\n",
        "    return quantized_tensor, regularization\n",
        "\n",
        "def uniform_quantization(tensor, num_bits=NUM_BITS):\n",
        "    min_val, max_val = tensor.min(), tensor.max()\n",
        "    scale = (max_val - min_val) / (2 ** num_bits - 1)\n",
        "    quantized_tensor = torch.round((tensor - min_val) / scale) * scale + min_val\n",
        "    return quantized_tensor\n",
        "\n",
        "def optimize_adaround(model, test_loader, num_iterations=100, lr=0.01):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    updated_state_dict = model.state_dict()\n",
        "    print(\"optimize started\")\n",
        "    for name, param in model.named_parameters():\n",
        "        if \"conv\" in name and \"weight\" in name:\n",
        "            print(name)\n",
        "            name = name.replace(\".weight\", \"\")\n",
        "            if name not in temp_activations:\n",
        "                continue  # Skip if activation was not stored\n",
        "            print(name)\n",
        "            v = torch.nn.Parameter(torch.zeros_like(param, device=device))\n",
        "            original_weight = param.clone().detach()\n",
        "            optimizer_v = optim.Adam([v], lr=lr)\n",
        "\n",
        "            #x = temp_activations[name.replace(\"weight\", \"\")]  # Get stored activation\n",
        "            x = temp_activations[name]\n",
        "            prev_loss = 100\n",
        "            for iter in range(num_iterations):\n",
        "                optimizer_v.zero_grad()\n",
        "                quantized_weight, reg_loss = adaround_round(original_weight, v)\n",
        "\n",
        "                quantized_output = nn.functional.conv2d(x, quantized_weight, stride=param.shape[2], padding=param.shape[3])\n",
        "                original_output = nn.functional.conv2d(x, original_weight, stride=param.shape[2], padding=param.shape[3])\n",
        "\n",
        "                loss = torch.norm(original_output - quantized_output, p='fro') ** 2 + reg_loss\n",
        "                if prev_loss < torch.norm(original_output - quantized_output, p='fro') ** 2:\n",
        "                  break\n",
        "                else :\n",
        "                  prev_loss = torch.norm(original_output - quantized_output, p='fro') ** 2\n",
        "                if iter%10==0:\n",
        "                  print(loss)\n",
        "                  print(torch.norm(original_output - quantized_output, p='fro') ** 2)\n",
        "                loss.backward()\n",
        "                optimizer_v.step()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                layer_name = name+\".weight\"\n",
        "                print(layer_name)\n",
        "                updated_state_dict[layer_name] = adaround_round(original_weight, v)[0]\n",
        "\n",
        "    model.load_state_dict(updated_state_dict)  # Ensure model actually uses quantized weights\n",
        "    print(\"AdaRound optimization complete. Model weights updated.\")\n",
        "\n",
        "# Run a forward pass to store activations\n",
        "with torch.no_grad():\n",
        "    for images, _ in test_loader:\n",
        "        images = images.to(device)\n",
        "        model(images)\n",
        "        break  # Only need a single batch\n",
        "\n",
        "# Apply AdaRound Quantization\n",
        "optimize_adaround(model, test_loader)\n",
        "\n",
        "# # Apply Uniform Quantization to Conv Layers Only\n",
        "# for name, param in model.named_parameters():\n",
        "#     if \"conv\" in name and \"weight\" in name:\n",
        "#         param.data = uniform_quantization(param.data)\n",
        "\n",
        "# Reload state dict to ensure updated weights are used\n",
        "model.load_state_dict(model.state_dict())\n",
        "\n",
        "# Evaluate Model after Quantization\n",
        "def evaluate(model, test_loader):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    print(f\"Test Accuracy after Quantization: {100 * correct / total:.2f}%\")\n",
        "\n",
        "evaluate(model, test_loader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JT413qlkoe-6",
        "outputId": "7453dc66-c512-41db-f617-a11d9b041039"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-deaa08a29cc2>:34: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load('/content/resnet18.pt', map_location=torch.device('cpu'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy after Quantization: 86.53%\n",
            "optimize started\n",
            "conv1.weight\n",
            "layer1.0.conv1.weight\n",
            "layer1.0.conv1\n",
            "tensor(4.7359, grad_fn=<AddBackward0>)\n",
            "tensor(1.0495, grad_fn=<PowBackward0>)\n",
            "tensor(3.8761, grad_fn=<AddBackward0>)\n",
            "tensor(0.3731, grad_fn=<PowBackward0>)\n",
            "tensor(3.6310, grad_fn=<AddBackward0>)\n",
            "tensor(0.2345, grad_fn=<PowBackward0>)\n",
            "tensor(3.4613, grad_fn=<AddBackward0>)\n",
            "tensor(0.1884, grad_fn=<PowBackward0>)\n",
            "tensor(3.3241, grad_fn=<AddBackward0>)\n",
            "tensor(0.1641, grad_fn=<PowBackward0>)\n",
            "tensor(3.2053, grad_fn=<AddBackward0>)\n",
            "tensor(0.1511, grad_fn=<PowBackward0>)\n",
            "tensor(3.0989, grad_fn=<AddBackward0>)\n",
            "tensor(0.1457, grad_fn=<PowBackward0>)\n",
            "layer1.0.conv1.weight\n",
            "layer1.0.conv2.weight\n",
            "layer1.0.conv2\n",
            "tensor(4.0245, grad_fn=<AddBackward0>)\n",
            "tensor(0.3381, grad_fn=<PowBackward0>)\n",
            "layer1.0.conv2.weight\n",
            "layer1.1.conv1.weight\n",
            "layer1.1.conv1\n",
            "tensor(4.7363, grad_fn=<AddBackward0>)\n",
            "tensor(1.0499, grad_fn=<PowBackward0>)\n",
            "tensor(3.9523, grad_fn=<AddBackward0>)\n",
            "tensor(0.4341, grad_fn=<PowBackward0>)\n",
            "tensor(3.6695, grad_fn=<AddBackward0>)\n",
            "tensor(0.2662, grad_fn=<PowBackward0>)\n",
            "tensor(3.4716, grad_fn=<AddBackward0>)\n",
            "tensor(0.2089, grad_fn=<PowBackward0>)\n",
            "tensor(3.3227, grad_fn=<AddBackward0>)\n",
            "tensor(0.1820, grad_fn=<PowBackward0>)\n",
            "tensor(3.1989, grad_fn=<AddBackward0>)\n",
            "tensor(0.1670, grad_fn=<PowBackward0>)\n",
            "tensor(3.0894, grad_fn=<AddBackward0>)\n",
            "tensor(0.1582, grad_fn=<PowBackward0>)\n",
            "tensor(2.9892, grad_fn=<AddBackward0>)\n",
            "tensor(0.1541, grad_fn=<PowBackward0>)\n",
            "layer1.1.conv1.weight\n",
            "layer1.1.conv2.weight\n",
            "layer1.1.conv2\n",
            "tensor(3.7670, grad_fn=<AddBackward0>)\n",
            "tensor(0.0806, grad_fn=<PowBackward0>)\n",
            "layer1.1.conv2.weight\n",
            "layer2.0.conv1.weight\n",
            "layer2.0.conv1\n",
            "tensor(8.7680, grad_fn=<AddBackward0>)\n",
            "tensor(1.3952, grad_fn=<PowBackward0>)\n",
            "tensor(7.6670, grad_fn=<AddBackward0>)\n",
            "tensor(0.7015, grad_fn=<PowBackward0>)\n",
            "tensor(7.1142, grad_fn=<AddBackward0>)\n",
            "tensor(0.4503, grad_fn=<PowBackward0>)\n",
            "tensor(6.7062, grad_fn=<AddBackward0>)\n",
            "tensor(0.3925, grad_fn=<PowBackward0>)\n",
            "tensor(6.3794, grad_fn=<AddBackward0>)\n",
            "tensor(0.3705, grad_fn=<PowBackward0>)\n",
            "tensor(6.0957, grad_fn=<AddBackward0>)\n",
            "tensor(0.3573, grad_fn=<PowBackward0>)\n",
            "layer2.0.conv1.weight\n",
            "layer2.0.conv2.weight\n",
            "layer2.0.conv2\n",
            "tensor(14.8721, grad_fn=<AddBackward0>)\n",
            "tensor(0.1265, grad_fn=<PowBackward0>)\n",
            "layer2.0.conv2.weight\n",
            "layer2.1.conv1.weight\n",
            "layer2.1.conv1\n",
            "tensor(14.8717, grad_fn=<AddBackward0>)\n",
            "tensor(0.1261, grad_fn=<PowBackward0>)\n",
            "layer2.1.conv1.weight\n",
            "layer2.1.conv2.weight\n",
            "layer2.1.conv2\n",
            "tensor(14.7789, grad_fn=<AddBackward0>)\n",
            "tensor(0.0333, grad_fn=<PowBackward0>)\n",
            "layer2.1.conv2.weight\n",
            "layer3.0.conv1.weight\n",
            "layer3.0.conv1\n",
            "tensor(29.6980, grad_fn=<AddBackward0>)\n",
            "tensor(0.2068, grad_fn=<PowBackward0>)\n",
            "layer3.0.conv1.weight\n",
            "layer3.0.conv2.weight\n",
            "layer3.0.conv2\n",
            "tensor(59.0010, grad_fn=<AddBackward0>)\n",
            "tensor(0.0186, grad_fn=<PowBackward0>)\n",
            "layer3.0.conv2.weight\n",
            "layer3.1.conv1.weight\n",
            "layer3.1.conv1\n",
            "tensor(58.9962, grad_fn=<AddBackward0>)\n",
            "tensor(0.0138, grad_fn=<PowBackward0>)\n",
            "layer3.1.conv1.weight\n",
            "layer3.1.conv2.weight\n",
            "layer3.1.conv2\n",
            "tensor(58.9901, grad_fn=<AddBackward0>)\n",
            "tensor(0.0077, grad_fn=<PowBackward0>)\n",
            "layer3.1.conv2.weight\n",
            "layer4.0.conv1.weight\n",
            "layer4.0.conv1\n",
            "tensor(117.9785, grad_fn=<AddBackward0>)\n",
            "tensor(0.0137, grad_fn=<PowBackward0>)\n",
            "layer4.0.conv1.weight\n",
            "layer4.0.conv2.weight\n",
            "layer4.0.conv2\n",
            "tensor(236.1194, grad_fn=<AddBackward0>)\n",
            "tensor(0.1898, grad_fn=<PowBackward0>)\n",
            "tensor(226.2509, grad_fn=<AddBackward0>)\n",
            "tensor(0.0861, grad_fn=<PowBackward0>)\n",
            "layer4.0.conv2.weight\n",
            "layer4.1.conv1.weight\n",
            "layer4.1.conv1\n",
            "tensor(239.7749, grad_fn=<AddBackward0>)\n",
            "tensor(3.8453, grad_fn=<PowBackward0>)\n",
            "tensor(228.2886, grad_fn=<AddBackward0>)\n",
            "tensor(2.3432, grad_fn=<PowBackward0>)\n",
            "tensor(217.1950, grad_fn=<AddBackward0>)\n",
            "tensor(1.4526, grad_fn=<PowBackward0>)\n",
            "tensor(207.3292, grad_fn=<AddBackward0>)\n",
            "tensor(1.1827, grad_fn=<PowBackward0>)\n",
            "layer4.1.conv1.weight\n",
            "layer4.1.conv2.weight\n",
            "layer4.1.conv2\n",
            "tensor(236.8526, grad_fn=<AddBackward0>)\n",
            "tensor(0.9230, grad_fn=<PowBackward0>)\n",
            "tensor(226.5841, grad_fn=<AddBackward0>)\n",
            "tensor(0.4953, grad_fn=<PowBackward0>)\n",
            "tensor(216.0872, grad_fn=<AddBackward0>)\n",
            "tensor(0.2546, grad_fn=<PowBackward0>)\n",
            "layer4.1.conv2.weight\n",
            "AdaRound optimization complete. Model weights updated.\n",
            "Test Accuracy after Quantization: 86.29%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(temp_activations.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "VcAYsxs9rZbS",
        "outputId": "65902286-8657-4f17-e0aa-902e7d174654"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'temp_activations' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-5fd902917b0d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_activations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'temp_activations' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import torch.utils.data as data\n",
        "import timm\n",
        "import numpy as np\n",
        "\n",
        "# Hyperparameters\n",
        "BATCH_SIZE = 128\n",
        "NUM_BITS = 3\n",
        "BETA = 2.0\n",
        "LAMBDA = 0.01  # Regularization weight\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, transform=transform, download=True)\n",
        "\n",
        "test_loader = data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# Load Pretrained ResNet18 from timm\n",
        "import resnet18\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = resnet18.resnet18(pretrained=False, device=device)\n",
        "model.to(device)\n",
        "\n",
        "state_dict = torch.load('/content/resnet18.pt', map_location=torch.device('cpu'))\n",
        "model.load_state_dict(state_dict, strict=False)\n",
        "model.eval()\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "\n",
        "def scaled_uniform_quantization(tensor, num_bits=NUM_BITS):\n",
        "    min_val, max_val = tensor.min(), tensor.max()\n",
        "\n",
        "    # Apply power function scaling (x^0.55)\n",
        "    tensor_sign = torch.sign(tensor)\n",
        "    tensor_scaled = tensor.abs() ** 0.55 * tensor_sign  # Preserve sign\n",
        "\n",
        "    scale = (tensor_scaled.max() - tensor_scaled.min()) / (2 ** num_bits - 1)\n",
        "    quantized_tensor = torch.round((tensor_scaled - tensor_scaled.min()) / scale) * scale + tensor_scaled.min()\n",
        "\n",
        "    # Descale back to original range\n",
        "    quantized_tensor = (quantized_tensor.abs() ** (1/0.55)) * torch.sign(quantized_tensor)\n",
        "\n",
        "    return quantized_tensor\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Apply AdaRound Quantization\n",
        "#optimize_adaround(model, test_loader)\n",
        "\n",
        "# Apply Uniform Quantization to Conv Layers Only\n",
        "for name, param in model.named_parameters():\n",
        "    if \"conv\" in name and \"weight\" in name:\n",
        "        param.data = scaled_uniform_quantization(param.data)\n",
        "\n",
        "# Evaluate Model after Quantization\n",
        "def evaluate(model, test_loader):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    print(f\"Test Accuracy after Quantization: {100 * correct / total:.2f}%\")\n",
        "\n",
        "evaluate(model, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YcEQtG4Drb3i",
        "outputId": "332ea1fe-00d0-438e-fd2a-8243dc66b384"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-f8092d78aee0>:34: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load('/content/resnet18.pt', map_location=torch.device('cpu'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy after Quantization: 80.88%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import torch.utils.data as data\n",
        "import timm\n",
        "import numpy as np\n",
        "\n",
        "# Hyperparameters\n",
        "BATCH_SIZE = 128\n",
        "NUM_BITS = 3\n",
        "BETA = 2.0\n",
        "LAMBDA = 0.01  # Regularization weight\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, transform=transform, download=True)\n",
        "\n",
        "test_loader = data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# Load Pretrained ResNet18 from timm\n",
        "import resnet18\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = resnet18.resnet18(pretrained=False, device=device)\n",
        "model.to(device)\n",
        "\n",
        "state_dict = torch.load('/content/resnet18.pt', map_location=torch.device('cpu'))\n",
        "model.load_state_dict(state_dict, strict=False)\n",
        "model.eval()\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "\n",
        "def offset_scaled_uniform_quantization(tensor, num_bits=NUM_BITS):\n",
        "    min_val, max_val = tensor.min(), tensor.max()\n",
        "\n",
        "    # Compute offset to center weights if necessary\n",
        "    offset = tensor.mean()\n",
        "    print(offset)\n",
        "    tensor_shifted = tensor - offset\n",
        "\n",
        "    # Apply power function scaling (x^0.55)\n",
        "    tensor_sign = torch.sign(tensor_shifted)\n",
        "    tensor_scaled = tensor_shifted.abs() ** 0.55 * tensor_sign  # Preserve sign\n",
        "\n",
        "    scale = (tensor_scaled.max() - tensor_scaled.min()) / (2 ** num_bits - 1)\n",
        "    quantized_tensor = torch.round((tensor_scaled - tensor_scaled.min()) / scale) * scale + tensor_scaled.min()\n",
        "\n",
        "    # Descale back to original range and apply offset correction\n",
        "    quantized_tensor = ((quantized_tensor.abs() ** (1/0.55)) * torch.sign(quantized_tensor)) + offset\n",
        "\n",
        "    return quantized_tensor\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Apply AdaRound Quantization\n",
        "#optimize_adaround(model, test_loader)\n",
        "\n",
        "# Apply Uniform Quantization to Conv Layers Only\n",
        "for name, param in model.named_parameters():\n",
        "    if \"conv\" in name and \"weight\" in name:\n",
        "        param.data = offset_scaled_uniform_quantization(param.data)\n",
        "\n",
        "# Evaluate Model after Quantization\n",
        "def evaluate(model, test_loader):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    print(f\"Test Accuracy after Quantization: {100 * correct / total:.2f}%\")\n",
        "\n",
        "evaluate(model, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uamYeS7F_En6",
        "outputId": "5de35fff-afb8-419e-e5d3-2d5abfb02a76"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-1-33d55d2dbdfa>:34: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load('/content/resnet18.pt', map_location=torch.device('cpu'))\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(-0.0006)\n",
            "tensor(-0.0005)\n",
            "tensor(-0.0004)\n",
            "tensor(-0.0004)\n",
            "tensor(-0.0005)\n",
            "tensor(-0.0006)\n",
            "tensor(-0.0005)\n",
            "tensor(-0.0005)\n",
            "tensor(-0.0005)\n",
            "tensor(-0.0002)\n",
            "tensor(-0.0002)\n",
            "tensor(-0.0002)\n",
            "tensor(-0.0003)\n",
            "tensor(-3.3380e-05)\n",
            "tensor(4.6454e-05)\n",
            "tensor(9.0216e-05)\n",
            "tensor(2.5724e-05)\n",
            "Test Accuracy after Quantization: 83.07%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import torch.utils.data as data\n",
        "import timm\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Hyperparameters\n",
        "BATCH_SIZE = 128\n",
        "NUM_BITS = 3\n",
        "BETA = 2.0\n",
        "LAMBDA = 0.01  # Regularization weight\n",
        "\n",
        "def compute_fisher_information(model, dataloader, device):\n",
        "    model.eval()\n",
        "    fisher_information = {}\n",
        "    for name, param in model.named_parameters():\n",
        "        if \"weight\" in name:\n",
        "            fisher_information[name] = torch.zeros_like(param, device=device)\n",
        "    counter = 0\n",
        "    for images, labels in dataloader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        model.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        for name, param in model.named_parameters():\n",
        "            if \"weight\" in name and param.grad is not None:\n",
        "                fisher_information[name] += (param.grad ** 2)\n",
        "        counter+=1\n",
        "        if counter==2:\n",
        "          break #only need 1 batch\n",
        "\n",
        "    return fisher_information\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, transform=transform, download=True)\n",
        "\n",
        "test_loader = data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "train_loader = data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# Load Pretrained ResNet18 from timm\n",
        "import resnet18\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = resnet18.resnet18(pretrained=False, device=device)\n",
        "model.to(device)\n",
        "\n",
        "state_dict = torch.load('/content/resnet18.pt', map_location=torch.device('cpu'))\n",
        "model.load_state_dict(state_dict, strict=False)\n",
        "model.eval()\n",
        "model.to(device)\n",
        "\n",
        "# Compute Fisher Information\n",
        "fisher_information = compute_fisher_information(model, train_loader, device)\n",
        "\n",
        "\n",
        "def uniform_quantization(tensor, num_bits=NUM_BITS, name=None):\n",
        "    min_val, max_val = tensor.min(), tensor.max()\n",
        "\n",
        "    # Compute Fisher Information sensitivity mask\n",
        "    if name and name in fisher_information:\n",
        "        fisher_info = fisher_information[name]\n",
        "        sensitivity_mask = fisher_info > torch.quantile(fisher_info, 0.3)\n",
        "        sensitive_weights = tensor[sensitivity_mask]\n",
        "    else:\n",
        "        sensitivity_mask = torch.abs(tensor) > torch.quantile(torch.abs(tensor), 0.95)\n",
        "        sensitive_weights = tensor[sensitivity_mask]\n",
        "\n",
        "    offset = sensitive_weights.mean() if sensitive_weights.numel() > 0 else tensor.mean()\n",
        "    print(offset)\n",
        "    tensor_shifted = tensor - offset\n",
        "\n",
        "    # # Apply sensitivity-aware clustering (k-means)\n",
        "    # tensor_flat = tensor_shifted.view(-1, 1).cpu().numpy()\n",
        "    # kmeans = KMeans(n_clusters=2 ** num_bits, n_init=10).fit(tensor_flat)\n",
        "    # clustered_tensor = torch.tensor(kmeans.cluster_centers_[kmeans.labels_], device=tensor.device).view(tensor.shape)\n",
        "\n",
        "    # Apply power function scaling (x^0.55)\n",
        "    # tensor_sign = torch.sign(clustered_tensor)\n",
        "    # tensor_scaled = clustered_tensor.abs() ** 0.55 * tensor_sign  # Preserve sign\n",
        "\n",
        "    tensor_sign = torch.sign(tensor_shifted)\n",
        "    tensor_scaled = tensor_shifted.abs() ** 0.55 * tensor_sign  # Preserve sign\n",
        "\n",
        "    scale = (tensor_scaled.max() - tensor_scaled.min()) / (2 ** num_bits - 1)\n",
        "    quantized_tensor = torch.round((tensor_scaled - tensor_scaled.min()) / scale) * scale + tensor_scaled.min()\n",
        "\n",
        "    # Descale back to original range and apply offset correction\n",
        "    quantized_tensor = ((quantized_tensor.abs() ** (1/0.55)) * torch.sign(quantized_tensor)) + offset\n",
        "\n",
        "    return quantized_tensor\n",
        "\n",
        "# Apply Uniform Quantization to Conv Layers Only\n",
        "for name, param in model.named_parameters():\n",
        "    if \"conv\" in name and \"weight\" in name:\n",
        "        param.data = uniform_quantization(param.data, name=name)\n",
        "\n",
        "# # Reload state dict to ensure updated weights are used\n",
        "# model.load_state_dict(model.state_dict())\n",
        "\n",
        "# Evaluate Model after Quantization\n",
        "def evaluate(model, test_loader):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    print(f\"Test Accuracy after Quantization: {100 * correct / total:.2f}%\")\n",
        "\n",
        "evaluate(model, test_loader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ThNk3xqcAvDA",
        "outputId": "f2460b91-ee99-45fd-fd34-0f323fc81918"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-15c5583f09e8>:60: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load('/content/resnet18.pt', map_location=torch.device('cpu'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(-0.0016)\n",
            "tensor(-0.0007)\n",
            "tensor(4.6154e-05)\n",
            "tensor(-0.0001)\n",
            "tensor(-0.0003)\n",
            "tensor(-0.0002)\n",
            "tensor(-0.0001)\n",
            "tensor(-0.0001)\n",
            "tensor(-0.0002)\n",
            "tensor(-2.7181e-05)\n",
            "tensor(-1.7936e-05)\n",
            "tensor(-0.0001)\n",
            "tensor(-0.0003)\n",
            "tensor(-1.2501e-05)\n",
            "tensor(9.9470e-05)\n",
            "tensor(0.0001)\n",
            "tensor(6.9517e-05)\n",
            "Test Accuracy after Quantization: 82.50%\n"
          ]
        }
      ]
    }
  ]
}